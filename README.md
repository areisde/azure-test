# IT-News Relevance & Ranking Service
[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)](LICENSE)

A serverless pipeline that **crawls tech-news feeds, filters for enterprise-IT relevance, ranks incidents by urgency**, and exposes them through HTTP & scheduled Azure Functions.

---

## ğŸ“ Project layout
```
.
â”œâ”€â”€ api/                    # Azure Function endpoints (HTTP + timer)
â”‚   â”œâ”€â”€ crawl.py            # Timer trigger â†’ run crawler every 6 hours
â”‚   â”œâ”€â”€ ingest.py           # Filters, scores and saves relevant articles to the DB
â”‚   â””â”€â”€ retrieve.py         # returns filtered & ranked articles
â”œâ”€â”€ services/               # core logic
â”‚   â”œâ”€â”€ crawler.py          # fetch RSS/Reddit sources
â”‚   â”œâ”€â”€ embeddings.py       # MiniLM encoder wrapper
â”‚   â”œâ”€â”€ filter.py           # Binary relevant article classifier + scoring functions
â”œâ”€â”€ prompts/                # zero-shot prompt templates for generating training data
â”œâ”€â”€ models/                 # .joblib weights for each binary classifier / filter
â”œâ”€â”€ sql/                    # SQL to execute on supabase to create the database
â”œâ”€â”€ db/                     # Database functions and models
â”‚   â”œâ”€â”€ crud.py             # Helper functions to fetch sources and upsert article batches
â”‚   â”œâ”€â”€ models.py           # Dataclass schemas (Source, Article) used throughout the app.
â”œâ”€â”€ tests/                  # pytest unit tests for retrieve and ingest endpoints
â”œâ”€â”€ scripts/                # Files used for preparing data and training models
â”‚   â”œâ”€â”€ label_dataset.ipynb # Label, train and save models
â”‚   â”œâ”€â”€ classifier.py       # Functions to label article using LLMs
â”œâ”€â”€ function_app.py         # Azure Functions entry-point
â””â”€â”€ host.json               # host configuration
```
---

## ğŸ” Filtering
Large-language models are *excellent* at spotting whether a news item matters to an IT manager (they understand â€œzero-dayâ€, â€œsev-1â€, â€œS3 outageâ€, etc.).  
**Problem:** each call is nondeterministic and expensive, which is unacceptable for an always-on pipeline. So, I took another approach.

### 1 Â· Synthetic data
* I first generated a dataset of 200 articles for training and 50 articles for testing on chatGPT using o3.
### 2 Â· Labelling
* **Zero-shot prompt** (`prompts/classifier/v1.txt`) is sent to GPT-4o to label each headline + first paragraph (if any) with four Boolean flags:  
  `relevant`, `severe`, `wide_scope`, `high_impact`.  
* 1-shot+examples prompt ensures consistent labels â†’ deterministic JSON.

### 3 Â· Embedding  
* **Sentence-Transformers `all-MiniLM-L6-v2`** (384-dim, frozen, L2-normalised).  
* Same vector is reused by *every* classifier â†’ single forward pass per article.

### 4 Â· Filter
| Flag | Model | Class weight | Why logistic? |
|------|-------|--------------|---------------|
| `relevant` | Logistic R. (`lbfgs`, C chosen by 5-fold GridSearchCV) | `balanced` | High recall with linear decision surface, easy to threshold. |

Final values stored in `models/relevant_model.joblib`, ensuring deterministic predictions.

---

## ğŸ“ˆ Ranking
In the same lofic as filtering, I trained three tiny â€œweak learnersâ€** â€“ plain logistic-regression heads â€“ on the frozen MiniLM embeddings.  
   *Theyâ€™re â€œweakâ€ in the boosting sense: individually simple, but in aggregate they capture enough signal.*

| Flag | Model | Class weight | Why logistic? |
|------|-------|--------------|---------------|
| `severe` | Logistic R. (`lbfgs`, C chosen by 5-fold GridSearchCV) | `balanced` | Separates â€œcriticalâ€ vs â€œroutineâ€ with few features. |
| `wide_scope` | Logistic R. (`lbfgs`, C chosen by 5-fold GridSearchCV) | `balanced` | Captures vendor keywords + semantic distance. |
| `high_impact` | Logistic R. (`lbfgs`, C chosen by 5-fold GridSearchCV) | `balanced` | Learns phrases like â€œmillions of recordsâ€. |

```text
importance_score =
    0.70 Ã— ( 0.50Â·P(severe)
           + 0.30Â·P(wide_scope)
           + 0.20Â·P(high_impact) )
  + 0.30 Ã— freshness
````
Elements are then ranked according to the importance_score

---

## ğŸš¦ Pipeline

1. **Entry points (two ways in)**  
   * **Timer trigger `crawl_sources`**  
     * Fires every **6 h** (`0 0 */6 * * *`).  
     * Calls `crawl.crawl_and_process()` â†’ pulls RSS feeds â†’ returns an **array of dicts**.  
   * **HTTP route `POST /api/ingest`**  
     * Accepts a JSON **array** *or* **ND-JSON** stream pushed by clients.

2. **Shared ingest routine `ingest.ingest_articles()`**  
   1. **Hydrate** each raw dict into `models.Article`.  
   2. **Relevance filter**  
      * MiniLM embedding â†’ `relevant_model.joblib`.  
      * Keep if `P(relevant) â‰¥ 0.55` (class-weight *balanced* â†’ high recall).  
   3. **Importance scoring**  
      | Head (model) | Predicts | File | Default weight |
      |--------------|----------|------|----------------|
      | `severe_model` | zero-day / sev-1 / CVSS â‰¥ 9 | `models/severe_model.joblib` | **0.50** |
      | `wide_scope_model` | tier-1 vendor affected | `models/wide_scope_model.joblib` | **0.30** |
      | `high_impact_model` | millions of users / global outage | `models/high_impact_model.joblib` | **0.20** |
      * **Freshness** =`e^(âˆ’age / 72 h)` (weight **0.30**).  
      * All four scores are stored on the `Article` object.  
   4. **Compute total**  
      ```text
        importance_score =
            0.70 Â· ( 0.50 Â· P(severe)
                   + 0.30 Â· P(wide_scope)
                   + 0.20 Â· P(high_impact) )
          + 0.30 Â· freshness
      ```

3. **Persist â€“ `crud.upload_articles()`**  
   * Batch **UPSERT** into Supabase `articles` table (idempotent on `id`).  
   * Columns include `severity_score`, `wide_scope_score`, `high_impact_score`, `importance_score`, `published_at`, etc.

4. **Retrieve â€“ `GET /api/retrieve`**  
   * `retrieve.retrieve_events()` runs  
     ```sql
     SELECT * FROM articles
     ORDER BY importance_score DESC, published_at DESC
     LIMIT $n;
     ```  
   * Each element in the returned array is a **JSON object** enriched with scores
produced by the pipeline:

        ```jsonc
        {
        "id": "https://www.computerweekly.com/news/366627640/â€¦",
        "title": "Luxury retailer LVMH says UK customer data was stolen in cyber attack",
        "body": "UK customers of luxury goods brand Louis Vuitton have been warned â€¦",
        "published_at": "2025-07-14T10:45:00Z",
        "created_at": "2025-07-15T09:41:19.7979Z",
        "source": "computerweekly",

        // model-generated signals  (0 â€“ 1 probabilities)
        "severity_score":     0.400745,
        "wide_scope_score":   0.585280,
        "high_impact_score":  0.859793,

        // final ranking value
        "score":   0.598855
        }

5. **Client usage**  
   * Dashboards, Slack bots, or SIEM webhooks hit **`/api/retrieve`** to obtain a ranked feed.  
   * All weights (`SEVERITY_WEIGHT`, `WIDE_SCOPE_WEIGHT`, `HIGH_IMPACT_WEIGHT`, `FRESHNESS_WEIGHT`) and the cron schedule are **overridable via environment variables**â€”no redeploy required.

---

## ğŸ“ˆ Pipeline diagram
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         internal call
â”‚ crawl_sources   â”‚       (timer function)
â”‚  - crawler.py   â”‚  <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Scheduler  (every 6 h)
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ raw dicts
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      
â”‚ ingest_articles       â”‚      HTTP POST /api/ingest
â”‚  (http + shared code) â”‚  <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Client pushes new batches                     
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             
      â”‚ Article dataclass objects               
      â–¼                                         
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ filter.relevant_articles  â”‚
â”‚  â€¢ MiniLM embedding       â”‚
â”‚  â€¢ relevant_model         â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ labels + vectors
      â–¼
 â”€ keep only â€œTrueâ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ importance_score    â”‚
â”‚  â€¢ severe_model     â”‚ 0.50 weight
â”‚  â€¢ wide_scope_model â”‚ 0.30 weight
â”‚  â€¢ high_impact_modelâ”‚ 0.20 weight
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ scores injected into Article objects
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ crud.upload_articles    â”‚  âœ Supabase `articles` table
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜





HTTP GET /api/retrieve
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  retrieve.py    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ ranking:
      â”‚   ORDER BY importance_score DESC
      â”‚   0.7*(0.5Â·severity + 0.3Â·scope + 0.2Â·impact) + 0.3Â·freshness
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JSON response   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

## ğŸ§° Tech Stack Overview

| Layer | Choice | Why |
|-------|--------|-----|
| **Serverless runtime** | **Azure Functions** | Native timer & HTTP triggers, zero-ops scaling, easy CI/CD |
| **Scheduler** | Azure Timer Trigger (`crawl_sources`, 6-hour cron) | Keeps crawler code and API in the same deployment unit. |
| **Database** | **Supabase (PostgreSQL)** | Instant REST API, row-level security, â€œupsertâ€ support, generous free tier for prototypes. Easy to set-up |
| **LLM providers** | OpenAI **GPT-4o** for one-off labelling Â· **o3** for synthetic headline generation | Best zero-shot quality (4o) and deterministic text generation (o3). |
| **ML library** | **scikit-learn** (LogisticRegression, GridSearchCV) | Fast, interpretable, deterministic. |
| **Embeddings** | `sentence-transformers/all-MiniLM-L6-v2` | 384-dim, CPU-friendly, MIT license. Easy to set-up |
| **Frontend** | **Nuxt 3** SPA, deployed as static files to **AWS S3 + CloudFront** <br>*(separate repo: `https://github.com/you/it-news-frontend`)* | CDN-backed delivery, zero server maintenance. |
| **CI / CD** | GitHub Actions Â· pytest Â· coverage badge | Lints, unit-tests, and auto-deploys to Azure on push to `main`. |

---

### ğŸ—„ï¸  Supabase schema

#### `sources` table
| column | type | note |
|--------|------|------|
| `id` *(PK)* | `uuid` | auto-gen |
| `name` | `text` | â€œAWS statusâ€, â€œTomâ€™s Hardwareâ€â€¦ |
| `url`  | `text` | Feed endpoint |
| `type` | `text` | `rss` / `json` |

#### `articles` table
| column | type | note |
|--------|------|------|
| `id` *(PK)* | `text` | canonical URL or GUID |
| `title` | `text` | headline |
| `body` | `text` | first paragraphs |
| `published_at` | `timestamptz` |
| `source` | `text` |
| `severity_score` | `float4` |
| `wide_scope_score` | `float4` |
| `high_impact_score` | `float4` |
| `created_at` | `timestamptz` | *default now()* |

---

## ğŸ“ˆ Evaluating filtering & ranking
Done inside `scripts/label_dataset.ipynb` after training each model and ranking algorithms.

| Layer | Method | Metric |
|-------|--------|--------|
| **Filtering (relevant / irrelevant)** | â€¢ Use GPT-4o once as an â€œoracleâ€ to relabel the test batch.<br>â€¢ Compare my modelâ€™s output â†’ **Precision / Recall** (recall is the KPI). | Target: **R â‰¥ 0.80**, P â‰¥ 0.70 |
| **Ranking inside the relevant set** | â€¢ Three independent LLMs score `severe`, `scope`, `impact`. <br>â€¢ Average â†’ ground-truth **importance_score**. <br>â€¢ Items above the median count as â€œpositiveâ€. | **Precision@5**, **nDCG@5**, MAP |

*This keeps evaluation deterministic, avoids manual labelling, and focuses on recall (donâ€™t miss critical news) while still checking that top-ranked items match the LLM consensus.*

### ğŸ”—  External repos

* **Backend (this repo):** serverless API, ML models, data pipeline  
* **Frontend:** [`it-news-frontend`](https://github.com/you/it-news-frontend) â€“ Nuxt 3 SPA that consumes `/api/retrieve` and renders a real-time dashboard.

---

## ğŸš€ Quick-start (local)

```bash
# 1. Clone & install
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2. Set env vars
cp env.example .env   # edit DB path, feed URLs, etc.

# 3. Run all functions locally
func start   # needs Azure Functions Core Tools â‰¥4

# 4. Trigger crawl manually
curl http://localhost:7071/api/crawl

Note: The timer trigger in function_app.py fires automatically every 6 h
(0 0 */6 * * *). Set run_on_startup=True while debugging.
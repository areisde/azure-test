{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac241dd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "680c7371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, average_precision_score\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "from scripts import classifier\n",
    "from services import embeddings, filter, crawler\n",
    "from db.models import Article\n",
    "from api import ingest, retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9e8b2",
   "metadata": {},
   "source": [
    "# Data\n",
    "Load synthetic training data generated using ChatGPT based on real world recent articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba4d1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 200 generated articles for training.\n",
      "There are 50 generated articles for testing.\n"
     ]
    }
   ],
   "source": [
    "data_types = [\"train\", \"test\"]\n",
    "articles = {}\n",
    "\n",
    "for data_type in data_types:\n",
    "    with open(f\"../data/{data_type}.json\", \"r\") as f:\n",
    "        articles[data_type] = json.load(f)\n",
    "    \n",
    "    print(f\"There are {len(articles[data_type])} generated articles for {data_type}ing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa435516",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "The goal is to leverage LLMs capacity for **zero-shot semantic understanding**, then use it to assign four independent labels to every headline/body pair:\n",
    "\n",
    "* **Relevance** – tag each article as *relevant* or *irrelevant* for enterprise IT teams.  \n",
    "* **Severity** – rate the technical danger (*severe* vs. *non-severe*; i.e. \"zero-day under active exploit\" vs \"patch released\").  \n",
    "* **Scope** – flag the breadth of the incident (*wide* vs. *narrow* scope; i.e. a high-tier vendor incident usually hits more organisations).  \n",
    "* **User impact** – gauge how many users / records are affected (*high* vs. *low* impact; i.e. \"millions of user records\" vs \"internal test\").\n",
    "\n",
    "The resulting label scores become the training targets for the filter and ranking models that will be promoted to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6386471b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling train articles: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [05:00<00:00,  1.50s/it]\n",
      "Labeling test articles: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:06<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# Label data with the LLM\n",
    "for data_type in data_types:\n",
    "    processed_articles = []\n",
    "    for article in tqdm(articles[data_type], desc=f\"Labeling {data_type} articles\"):\n",
    "        result = classifier.classify_article(article)\n",
    "        processed_article = {\n",
    "            \"article\": article,\n",
    "            \"classification\": result\n",
    "        }\n",
    "        processed_articles.append(processed_article)\n",
    "    \n",
    "    # Save labelled data\n",
    "    with open(f'../data/{data_type}_labelled.json', 'w') as f:\n",
    "        json.dump(processed_articles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d82e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 relevant articles in training dataset.\n",
      "Found 26 relevant articles in testing dataset.\n"
     ]
    }
   ],
   "source": [
    "labelled_articles = {}\n",
    "for data_type in data_types:\n",
    "    with open(f\"../data/{data_type}_labelled.json\", \"r\") as f:\n",
    "        processed_articles = json.load(f)\n",
    "\n",
    "    labelled_articles[data_type] = processed_articles\n",
    "    relevant_articles = [article for article in processed_articles if article['classification'].get('relevant')]\n",
    "    print(f\"Found {len(relevant_articles)} relevant articles in {data_type}ing dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83601a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Okta confirms a breach exposing 73 K session tokens; password resets underway', 'body': 'Okta acknowledged that threat actors exfiltrated 73 000 session tokens via a compromised support account. The company has forced global password resets.', 'published_at': '2025-07-13T22:48:09Z', 'id': 'https://example.com/388c8aa1-5aae-4799-adc0-8191f4397e0e', 'created_at': '2025-07-13T22:50:09Z', 'source': 'Okta', 'relevant': True}\n"
     ]
    }
   ],
   "source": [
    "# Example of relevant article\n",
    "print(relevant_articles[2]['article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ecaee",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679fb7a1",
   "metadata": {},
   "source": [
    "### Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dddd669-b0a9-4e09-819b-2c18f06ab55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [text['article']['title'] + text['article']['body'] for text in labelled_articles[\"train\"]] # Only keep main text from articles for embedding\n",
    "y = {}\n",
    "y[\"relevant\"] = [1 if article['classification'].get('relevant') else 0 for article in labelled_articles[\"train\"] ] # Label for relevant parameter\n",
    "y[\"severe\"]  = [1 if article['classification'].get('severe') else 0 for article in labelled_articles[\"train\"] ] # Label for severe parameter\n",
    "y[\"wide_scope\"]  = [1 if article['classification'].get('wide_scope') else 0 for article in labelled_articles[\"train\"] ] # Label for wide_scope parameter\n",
    "y[\"high_impact\"]  = [1 if article['classification'].get('high_impact') else 0 for article in labelled_articles[\"train\"] ] # Label for high_impact parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e8e58",
   "metadata": {},
   "source": [
    "### Set model up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d775ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME   = \"all-MiniLM-L6-v2\"\n",
    "BATCH_SIZE   = 32\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed7baa",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05ec530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  4.43it/s]\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "def encode_texts(texts):\n",
    "    return embedder.encode(\n",
    "        texts,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "\n",
    "# Produce embeddings \n",
    "X_encoded = encode_texts(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ba380",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e94f2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\"relevant\", \"severe\", \"wide_scope\", \"high_impact\"]\n",
    "param_grid = {'C': [0.25, 0.5, 1.0, 2.0, 4.0]}\n",
    "X_test_dict = {}\n",
    "y_test_dict = {}\n",
    "\n",
    "for param in params:\n",
    "    # Train / test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_encoded, y[param], test_size=0.20, stratify=y[param], random_state=SEED\n",
    "    )\n",
    "    X_test_dict[param] = X_test\n",
    "    y_test_dict[param] = y_test\n",
    "\n",
    "    # Class weights\n",
    "    \"\"\"\n",
    "    We balance classes since the main class is usually less\n",
    "    represented in the dataset.\n",
    "    \"\"\"\n",
    "    class_weight = 'balanced'\n",
    "\n",
    "    # Model\n",
    "    lr = LogisticRegression(\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        random_state=SEED,\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='f1', n_jobs=1)\n",
    "\n",
    "    # Fit\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    clf = grid_search.best_estimator_\n",
    "\n",
    "    # Save model weights\n",
    "    joblib.dump(clf, f\"../models/{param}_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd6618b",
   "metadata": {},
   "source": [
    "### Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bdadf35-c957-48ad-bd36-f01af49adc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process test data\n",
    "X_test_embedded = encode_texts(labelled_articles[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c85ba6c-4bc4-4e34-8b06-af33545f805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_relevant = [a[\"classification\"][\"relevant\"] for a in labelled_articles[\"test\"]]\n",
    "relevant_mask = np.asarray(y_relevant, dtype=bool)\n",
    "articles_arr = np.array(labelled_articles[\"test\"], dtype=object)\n",
    "relevant_articles = articles_arr[relevant_mask].tolist()\n",
    "X_relevant = X_test_embedded[relevant_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13def816-6803-41e3-9d9d-4528d0ecb5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(param):\n",
    "    if param == \"relevant\":\n",
    "        X = X_test_embedded\n",
    "        y_gt = y_relevant\n",
    "    else:\n",
    "        X = X_relevant\n",
    "        y_gt = [a[\"classification\"][param] for a in relevant_articles]\n",
    "        \n",
    "    clf = joblib.load(f\"../models/{param}_model.joblib\")\n",
    "    y_proba = clf.predict_proba(X)[:, 1]\n",
    "    threshold = 0.55 if param == \"relevant\" else 0.5\n",
    "    y_pred = (y_proba >= threshold)\n",
    "    print(classification_report(y_gt, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d16ac185-08b0-4e33-83de-6f015eef6044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating performance for 'relevant' parameter...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.958     0.958     0.958        24\n",
      "        True      0.962     0.962     0.962        26\n",
      "\n",
      "    accuracy                          0.960        50\n",
      "   macro avg      0.960     0.960     0.960        50\n",
      "weighted avg      0.960     0.960     0.960        50\n",
      "\n",
      "Evaluating performance for 'severe' parameter...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.583     0.636     0.609        11\n",
      "        True      0.714     0.667     0.690        15\n",
      "\n",
      "    accuracy                          0.654        26\n",
      "   macro avg      0.649     0.652     0.649        26\n",
      "weighted avg      0.659     0.654     0.655        26\n",
      "\n",
      "Evaluating performance for 'wide_scope' parameter...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      1.000     0.143     0.250         7\n",
      "        True      0.760     1.000     0.864        19\n",
      "\n",
      "    accuracy                          0.769        26\n",
      "   macro avg      0.880     0.571     0.557        26\n",
      "weighted avg      0.825     0.769     0.698        26\n",
      "\n",
      "Evaluating performance for 'high_impact' parameter...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.846     0.647     0.733        17\n",
      "        True      0.538     0.778     0.636         9\n",
      "\n",
      "    accuracy                          0.692        26\n",
      "   macro avg      0.692     0.712     0.685        26\n",
      "weighted avg      0.740     0.692     0.700        26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param in params:\n",
    "    print(f\"Evaluating performance for '{param}' parameter...\")\n",
    "    evaluate(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07efac92-eba0-4962-8fa7-21a783313b5b",
   "metadata": {},
   "source": [
    "# Ranking\n",
    "For filtering articles, we will use the \"relevant\" classifier that simply states if an article is relevant for an IT professional or not.\n",
    "\n",
    "For ranking articles by importance we will use a mix of the three other classifiers to build an importance score together with the freshness of the articles (published_at).\n",
    "Then, arbitrary weights were picked for the different parameters, based on a personal evaluation of what should be put forward :\n",
    "* Severity : 0.5\n",
    "* Wide scope : 0.3\n",
    "* High impact : 0.2\n",
    "\n",
    "The **final importance score** is calculated as a weighted sum of the three dimensions:\n",
    "\n",
    "```\n",
    "Importance Score = (Severity × 0.5) + (Wide Scope × 0.3) + (High Impact × 0.2)\n",
    "```\n",
    "On top of this, score is further biased by its freshness (how many hours ago was the article published), the hottest the higher the importance. Again, we arbitrarly pick the wieghts to be :\n",
    "* Parameters : 0.7\n",
    "* Freshness : 0.3\n",
    "\n",
    "```\n",
    "Final Score = (Importance Score × 0.7) + (Freshness × 0.3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866bfa4e-7027-4c82-acc7-89e1d71504e2",
   "metadata": {},
   "source": [
    "### Rank the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "71012d9c-35d4-4fbd-9580-56b35bd2ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute importance scores\n",
    "simple_relevant_articles = [a[\"article\"] for a in relevant_articles]\n",
    "scored_articles = filter.importance_score(simple_relevant_articles, X_relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "118edb30-76ba-4d9a-b0d5-4b0083c5b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_articles, final_score = retrieve.rank(scored_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6c1227fe-3c61-4c30-8660-cd5628f9efa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'title': 'Researchers spotted a zero-day exploit kit abusing CVE-2025-9911 in SSL-VPN gateways',\n",
       "   'body': 'Researchers spotted an active zero-day exploit kit abusing CVE-2025-9911 in several SSL-VPN gateways. Administrators are urged to apply interim mitigations.',\n",
       "   'published_at': '2025-07-14T06:17:12Z',\n",
       "   'id': 'https://example.com/1d1a4e62-fa2b-47cd-8f6a-1f3af741e601',\n",
       "   'created_at': '2025-07-14T06:19:12Z',\n",
       "   'source': 'Cisco Talos',\n",
       "   'relevant': True,\n",
       "   'severity_score': 0.6780227720644709,\n",
       "   'wide_scope_score': 0.657861322159996,\n",
       "   'high_impact_score': 0.5298952333226373},\n",
       "  0.6555247439191663),\n",
       " ({'title': 'AWS is triaging a sev-1 outage in eu-south-1 affecting S3 and SNS',\n",
       "   'body': 'AWS reports a sev-1 outage in eu-south-1, impacting S3 and SNS APIs. Engineers are rolling back a faulty networking update.',\n",
       "   'published_at': '2025-07-14T04:02:45Z',\n",
       "   'id': 'https://example.com/24b7c7ce-0b3c-4dd2-9b9e-5a64afae0f02',\n",
       "   'created_at': '2025-07-14T04:04:45Z',\n",
       "   'source': 'AWS',\n",
       "   'relevant': True,\n",
       "   'severity_score': 0.5731812118682675,\n",
       "   'wide_scope_score': 0.5919132797717263,\n",
       "   'high_impact_score': 0.7277678009304719},\n",
       "  0.6263743962102691),\n",
       " ({'title': 'CISA adds SonicWall CVE-2025-2879 to KEV catalog; federal agencies must patch by July 20',\n",
       "   'body': 'CVE-2025-2879 is an auth-bypass flaw in SonicOS. Agencies have five days to comply.',\n",
       "   'published_at': '2025-07-14T07:42:08Z',\n",
       "   'id': 'https://example.com/6d1d2a2b-53ae-4d94-9d53-6d4598e71029',\n",
       "   'created_at': '2025-07-14T07:44:08Z',\n",
       "   'source': 'CISA',\n",
       "   'relevant': True,\n",
       "   'severity_score': 0.5552130471677397,\n",
       "   'wide_scope_score': 0.6041954207753785,\n",
       "   'high_impact_score': 0.5678699434401399},\n",
       "  0.6106757259505806)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_articles[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3d853-8e9c-47d6-939f-943a6ba11018",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ffc14050-0c4a-4698-aa6b-626060bf121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_articles = []\n",
    "\n",
    "for art in relevant_articles:\n",
    "    new_art = {\n",
    "        'id' : art['article']['id'],\n",
    "        'title' : art['article']['title'],\n",
    "        'body' : art['article']['body'],\n",
    "        'published_at' : art['article']['published_at'],\n",
    "        'severity_score' : 1 if art['classification']['severe'] else 0,\n",
    "        'wide_scope_score' : 1 if art['classification']['wide_scope'] else 0,\n",
    "        'high_impact_score' : 1 if art['classification']['high_impact'] else 0\n",
    "    }\n",
    "    gt_articles.append(new_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bd13f26e-f1a8-43c8-b968-5b65a4521b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_gt, final_score_gt = retrieve.rank(gt_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "78b7ce57-dca7-4d6f-b562-e53d78ec5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {art['id'] : score for art, score in ranked_articles}\n",
    "gt_dict = {art['id'] : score for art, score in ranked_gt}\n",
    "\n",
    "# Align articles by id\n",
    "k_vals = [5, 10, 15]\n",
    "thresh = 0.5 # treat gt > 0.5 as \"relevant\"\n",
    "ids = sorted(set(pred_dict) & set(gt_dict))\n",
    "y_pred = np.array([pred_dict[i] for i in common_ids])\n",
    "y_true = (np.array([gt_dict[i] for i in ids]) > thresh).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "664db6af-bf33-4dd4-a9d6-01090a008959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@5: 0.800   R@5: 0.235\n",
      "P@10: 0.700   R@10: 0.412\n",
      "P@15: 0.667   R@15: 0.588\n"
     ]
    }
   ],
   "source": [
    "total_relevant = y_true.sum()\n",
    "\n",
    "for k in k_vals:\n",
    "    topk_idx = np.argsort(y_pred)[-k:][::-1]     # indices of k highest scores\n",
    "    rel_in_topk = y_true[topk_idx].sum()\n",
    "\n",
    "    prec_k = rel_in_topk / k\n",
    "    rec_k  = rel_in_topk / total_relevant if total_relevant else 0\n",
    "\n",
    "    print(f\"P@{k}: {prec_k:0.3f}   R@{k}: {rec_k:0.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

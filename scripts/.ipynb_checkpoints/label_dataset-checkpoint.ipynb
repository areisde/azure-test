{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac241dd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "680c7371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import datetime as dt\n",
    "\n",
    "sys.path.append('..')\n",
    "from scripts import classifier\n",
    "from services import embeddings, filter\n",
    "from db.models import Article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9e8b2",
   "metadata": {},
   "source": [
    "# Data\n",
    "Load synthetic training data generated using ChatGPT based on real world recent articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ba4d1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 200 generated articles.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/train.json\", \"r\") as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "print(f\"There are {len(generated_articles)} generated articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa435516",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "The goal is to leverage LLMs capacity for **zero-shot semantic understanding**, then use it to assign four independent labels to every headline/body pair:\n",
    "\n",
    "* **Relevance** – tag each article as *relevant* or *irrelevant* for enterprise IT teams.  \n",
    "* **Severity** – rate the technical danger (*severe* vs. *non-severe*; i.e. \"zero-day under active exploit\" vs \"patch released\").  \n",
    "* **Scope** – flag the breadth of the incident (*wide* vs. *narrow* scope; i.e. a high-tier vendor incident usually hits more organisations).  \n",
    "* **User impact** – gauge how many users / records are affected (*high* vs. *low* impact; i.e. \"millions of user records\" vs \"internal test\").\n",
    "\n",
    "The resulting label scores become the training targets for the filter and ranking models that will be promoted to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6386471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_articles = []\n",
    "for article in articles:\n",
    "    result = classifier.classify_article(article)\n",
    "    processed_article = {\n",
    "        \"article\": article,\n",
    "        \"classification\": result\n",
    "    }\n",
    "    processed_articles.append(processed_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "007b645e-0784-4dca-bd31-ef70406c8a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save labelled data\n",
    "with open('../data/train_labelled.json', 'w') as f:\n",
    "    json.dump(processed_articles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61d82e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 relevant articles.\n"
     ]
    }
   ],
   "source": [
    "relevant_articles = [article for article in processed_articles if article['classification'].get('relevant')]\n",
    "print(f\"Found {len(relevant_articles)} relevant articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83601a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Elastic confirms a data breach exposing 488K customer records; incident response is underw', 'body': 'Elastic confirms a data breach exposing 488K customer records; incident response is underway.', 'published_at': '2025-07-12T05:18:14Z', 'id': 'https://example.com/75919e72-8cd3-48aa-a104-65de43370ef7', 'created_at': '2025-07-12T05:20:14Z', 'source': 'Elastic', 'relevant': True}\n"
     ]
    }
   ],
   "source": [
    "# Example of relevant article\n",
    "print(relevant_articles[2]['article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ecaee",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679fb7a1",
   "metadata": {},
   "source": [
    "### Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9dddd669-b0a9-4e09-819b-2c18f06ab55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [text['article']['title'] + text['article']['body'] for text in processed_articles] # Only keep main text from articles for embedding\n",
    "y = {}\n",
    "y[\"relevant\"] = [1 if article['classification'].get('relevant') else 0 for article in processed_articles ] # Label for relevant parameter\n",
    "y[\"severe\"]  = [1 if article['classification'].get('severe') else 0 for article in processed_articles ] # Label for severe parameter\n",
    "y[\"wide_scope\"]  = [1 if article['classification'].get('wide_scope') else 0 for article in processed_articles ] # Label for wide_scope parameter\n",
    "y[\"high_impact\"]  = [1 if article['classification'].get('high_impact') else 0 for article in processed_articles ] # Label for high_impact parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e8e58",
   "metadata": {},
   "source": [
    "### Set model up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d775ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME   = \"all-MiniLM-L6-v2\"\n",
    "BATCH_SIZE   = 32\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed7baa",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05ec530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "def encode_texts(texts):\n",
    "    return embedder.encode(\n",
    "        texts,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "\n",
    "# Produce embeddings \n",
    "X_encoded = encode_texts(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ba380",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e94f2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\"relevant\", \"severe\", \"wide_scope\", \"high_impact\"]\n",
    "param_grid = {'C': [0.5, 1.0, 2.0]}\n",
    "X_test_dict = {}\n",
    "y_test_dict = {}\n",
    "\n",
    "for param in params:\n",
    "    # Train / test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_encoded, y[param], test_size=0.20, stratify=y[param], random_state=SEED\n",
    "    )\n",
    "    X_test_dict[param] = X_test\n",
    "    y_test_dict[param] = y_test\n",
    "\n",
    "    # Class weights\n",
    "    \"\"\"\n",
    "    We bias the classifier towards a higher recall in case relevancy\n",
    "    is at stake (not to miss important articles)\n",
    "    \"\"\"\n",
    "    class_weight = {0:1, 1:2} if param == \"relevant\" else {0:1, 1:1}\n",
    "\n",
    "    # Model\n",
    "    lr = LogisticRegression(\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        random_state=SEED,\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='f1', n_jobs=1)\n",
    "\n",
    "    # Fit\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    clf = grid_search.best_estimator_\n",
    "\n",
    "    # Save model weights\n",
    "    joblib.dump(clf, f\"../models/{param}_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd6618b",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13def816-6803-41e3-9d9d-4528d0ecb5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(param):\n",
    "    clf = joblib.load(f\"../models/{param}_model.joblib\")\n",
    "    y_pred = clf.predict(X_test_dict[param])\n",
    "    print(classification_report(y_test_dict[param], y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d16ac185-08b0-4e33-83de-6f015eef6044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating performance for 'relevant' parameter...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        20\n",
      "           1      1.000     1.000     1.000        20\n",
      "\n",
      "    accuracy                          1.000        40\n",
      "   macro avg      1.000     1.000     1.000        40\n",
      "weighted avg      1.000     1.000     1.000        40\n",
      "\n",
      "Evaluating performance for 'severe' parameter...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        26\n",
      "           1      1.000     1.000     1.000        14\n",
      "\n",
      "    accuracy                          1.000        40\n",
      "   macro avg      1.000     1.000     1.000        40\n",
      "weighted avg      1.000     1.000     1.000        40\n",
      "\n",
      "Evaluating performance for 'wide_scope' parameter...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.870     0.930        23\n",
      "           1      0.850     1.000     0.919        17\n",
      "\n",
      "    accuracy                          0.925        40\n",
      "   macro avg      0.925     0.935     0.925        40\n",
      "weighted avg      0.936     0.925     0.925        40\n",
      "\n",
      "Evaluating performance for 'high_impact' parameter...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.933     0.903     0.918        31\n",
      "           1      0.700     0.778     0.737         9\n",
      "\n",
      "    accuracy                          0.875        40\n",
      "   macro avg      0.817     0.841     0.827        40\n",
      "weighted avg      0.881     0.875     0.877        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param in params:\n",
    "    print(f\"Evaluating performance for '{param}' parameter...\")\n",
    "    evaluate(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07efac92-eba0-4962-8fa7-21a783313b5b",
   "metadata": {},
   "source": [
    "# Ranking\n",
    "For filtering articles, we will use the \"relevant\" classifier that simply states if an article is relevant for an IT professional or not.\n",
    "\n",
    "For ranking articles by importance we will use a mix of the three other classifiers to build an importance score together with the freshness of the articles (published_at).\n",
    "Then, arbitrary weights were picked for the different parameters, based on a personal evaluation of what should be put forward :\n",
    "* Severity : 0.5\n",
    "* Wide scope : 0.3\n",
    "* High impact : 0.2\n",
    "\n",
    "The **final importance score** is calculated as a weighted sum of the three dimensions:\n",
    "\n",
    "```\n",
    "Importance Score = (Severity × 0.5) + (Wide Scope × 0.3) + (High Impact × 0.2)\n",
    "```\n",
    "On top of this, score is further biased by its freshness (how many hours ago was the article published), the hottest the higher the importance. Again, we arbitrarly pick the wieghts to be :\n",
    "* Parameters : 0.7\n",
    "* Freshness : 0.3\n",
    "\n",
    "```\n",
    "Final Score = (Importance Score × 0.7) + (Freshness × 0.3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "63e79ccb-f7c1-462f-9ada-832eb75624d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_score(article):\n",
    "    # Embed article\n",
    "    text = article['title'] + article['body'].split(\".\")[0]\n",
    "    article_embedded = embedder.encode(text, normalize_embeddings=True, convert_to_numpy=True)\n",
    "\n",
    "    # Score based on parameters\n",
    "    params = [\"severe\", \"wide_scope\", \"high_impact\"]\n",
    "    weights = {\"severe\" : 0.5, \"wide_scope\" : 0.3, \"high_impact\" : 0.2}\n",
    "    scores = {}\n",
    "    \n",
    "    for param in params:\n",
    "        clf = joblib.load(f\"../models/{param}_model.joblib\")\n",
    "        y_pred = clf.predict_proba(article_embedded.reshape(1,-1))\n",
    "        y_prob = y_pred[0][1] # Probability of article being severe or wide_scope or high_impact\n",
    "        scores[param] = y_prob\n",
    "\n",
    "    score = sum([weights[param]*scores[param] for param in params])\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e6c803c4-fb81-4427-aa14-1b5e31209a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freshness_score(article):\n",
    "    published_at = article['published_at']\n",
    "    published = dt.datetime.fromisoformat(published_at.replace(\"Z\", \"+00:00\"))\n",
    "    now_utc = dt.datetime.now(dt.timezone.utc)\n",
    "    age_hours = (now_utc - published).total_seconds() / 3600\n",
    "    tau_hours = 72\n",
    "    freshness_score = math.exp(-age_hours / tau_hours)\n",
    "\n",
    "    return freshness_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866bfa4e-7027-4c82-acc7-89e1d71504e2",
   "metadata": {},
   "source": [
    "### Rank the 10 first relevant articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9e4a230a-aecb-46e3-a810-4c2ba5238b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_scored = {}\n",
    "\n",
    "for article in relevant_articles[:10]:\n",
    "    article_content = article['article']\n",
    "    score = importance_score(article_content)*0.7 + freshness_score(article_content)*0.3\n",
    "\n",
    "    articles_scored[article_content['title']] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "42e0d5d5-f496-4fbd-a037-76b7aa931d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Researchers discovered a new exploit kit targeting SSL‑VPN, weaponizing CVE-2025-1166': 0.6480853023311397,\n",
       " 'Researchers discovered a new exploit kit targeting ASA, weaponizing CVE-2024-7924': 0.6304578930678473,\n",
       " 'Elastic confirms a data breach exposing 488K customer records; incident response is underw': 0.3978406457143619,\n",
       " 'Elastic is investigating a critical vulnerability (CVE-2025-2638) allowing remote code exe': 0.6238727570306012,\n",
       " 'Researchers discovered a new exploit kit targeting ESXi, weaponizing CVE-2025-8484': 0.6076048163854597,\n",
       " 'Cloudflare confirms a data breach exposing 383K customer records; incident response is und': 0.4142366318348899,\n",
       " 'Cisco is investigating a major vulnerability (CVE-2024-5335) allowing remote code executio': 0.6000597649246994,\n",
       " 'Atlassian reports a critical outage in us-west-2, affecting services Cloud Functions, Conf': 0.6014066016557043,\n",
       " 'Researchers discovered a new exploit kit targeting ESXi, weaponizing CVE-2024-5681': 0.6177824458355199,\n",
       " 'AWS is investigating a sev‑1 vulnerability (CVE-2025-4150) allowing remote code execution': 0.649494431769017}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6f14facf-9315-4103-9dd3-f89313fe2c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AWS is investigating a sev‑1 vulnerability (CVE-2025-4150) allowing remote code execution',\n",
       "  0.649494431769017),\n",
       " ('Researchers discovered a new exploit kit targeting SSL‑VPN, weaponizing CVE-2025-1166',\n",
       "  0.6480853023311397),\n",
       " ('Researchers discovered a new exploit kit targeting ASA, weaponizing CVE-2024-7924',\n",
       "  0.6304578930678473),\n",
       " ('Elastic is investigating a critical vulnerability (CVE-2025-2638) allowing remote code exe',\n",
       "  0.6238727570306012),\n",
       " ('Researchers discovered a new exploit kit targeting ESXi, weaponizing CVE-2024-5681',\n",
       "  0.6177824458355199),\n",
       " ('Researchers discovered a new exploit kit targeting ESXi, weaponizing CVE-2025-8484',\n",
       "  0.6076048163854597),\n",
       " ('Atlassian reports a critical outage in us-west-2, affecting services Cloud Functions, Conf',\n",
       "  0.6014066016557043),\n",
       " ('Cisco is investigating a major vulnerability (CVE-2024-5335) allowing remote code executio',\n",
       "  0.6000597649246994),\n",
       " ('Cloudflare confirms a data breach exposing 383K customer records; incident response is und',\n",
       "  0.4142366318348899),\n",
       " ('Elastic confirms a data breach exposing 488K customer records; incident response is underw',\n",
       "  0.3978406457143619)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked = sorted(\n",
    "    articles_scored.items(),\n",
    "    key=lambda kv: kv[1],\n",
    "    reverse=True\n",
    ")\n",
    "ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3fed28a6-c246-4991-8dc6-5f3d2a205bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_scoring = filter.importance_score(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b986cc7e-5357-404f-a576-a39d8bcf554a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72996899, 0.69068082, 0.3795    , 0.70170198, 0.10949218,\n",
       "       0.67710699, 0.39411317, 0.69544727, 0.11875254, 0.68763295,\n",
       "       0.14147231, 0.69787317, 0.13175418, 0.0926825 , 0.09860725,\n",
       "       0.72583878, 0.53916547, 0.12180034, 0.69800088, 0.35151447,\n",
       "       0.76506029, 0.10219338, 0.1765807 , 0.70917371, 0.04735758,\n",
       "       0.35836903, 0.12189244, 0.08219349, 0.11414033, 0.70145648,\n",
       "       0.69653625, 0.38863925, 0.07655671, 0.10786138, 0.08262175,\n",
       "       0.62148038, 0.06462057, 0.66902113, 0.71002945, 0.08911256,\n",
       "       0.10029471, 0.69946482, 0.09048023, 0.12382677, 0.1141191 ,\n",
       "       0.68291969, 0.71226652, 0.04506325, 0.69314487, 0.73480273,\n",
       "       0.05325617, 0.69946472, 0.09007586, 0.04669457, 0.70276765,\n",
       "       0.05325617, 0.65393534, 0.09377526, 0.0489991 , 0.3624135 ,\n",
       "       0.66466991, 0.65300519, 0.48271188, 0.20728033, 0.72028757,\n",
       "       0.13811612, 0.6942418 , 0.6647865 , 0.38224053, 0.04453754,\n",
       "       0.42604356, 0.08194406, 0.04669457, 0.11478874, 0.10822914,\n",
       "       0.04506325, 0.06432777, 0.378411  , 0.65228133, 0.68736682,\n",
       "       0.66373115, 0.10259292, 0.66988509, 0.35577186, 0.66074252,\n",
       "       0.68022427, 0.10092368, 0.68514528, 0.04735758, 0.38721467,\n",
       "       0.61999678, 0.12258548, 0.650385  , 0.38230016, 0.74102579,\n",
       "       0.62825212, 0.04735758, 0.09407103, 0.11131989, 0.63395256,\n",
       "       0.08714201, 0.14147231, 0.70266119, 0.67929396, 0.09377526,\n",
       "       0.67063863, 0.60416346, 0.06833258, 0.04669457, 0.29448065,\n",
       "       0.37310485, 0.14717275, 0.13753989, 0.71476028, 0.04735758,\n",
       "       0.12427096, 0.17429309, 0.12782679, 0.1046177 , 0.12410735,\n",
       "       0.14561364, 0.39367174, 0.70332813, 0.35134335, 0.09007586,\n",
       "       0.14900787, 0.38239788, 0.612624  , 0.38400379, 0.07815244,\n",
       "       0.04376491, 0.41101673, 0.71819143, 0.70703372, 0.40084713,\n",
       "       0.34617221, 0.1175047 , 0.11220836, 0.18196405, 0.69642341,\n",
       "       0.70948212, 0.6234445 , 0.37738855, 0.71867482, 0.14836388,\n",
       "       0.19526299, 0.65695746, 0.05799135, 0.12652114, 0.1333272 ,\n",
       "       0.7230204 , 0.11596345, 0.04376491, 0.14869689, 0.64583611,\n",
       "       0.09616116, 0.15074824, 0.10786138, 0.17258571, 0.62201727,\n",
       "       0.68879904, 0.6515594 , 0.09931747, 0.09048023, 0.0859925 ,\n",
       "       0.22304725, 0.09633449, 0.05799135, 0.72257044, 0.12837687,\n",
       "       0.73747675, 0.17657317, 0.63331461, 0.06432777, 0.09243304,\n",
       "       0.75834933, 0.14043152, 0.1268451 , 0.0489991 , 0.65674175,\n",
       "       0.08833842, 0.38595903, 0.69003023, 0.45403739, 0.62691592,\n",
       "       0.65872101, 0.10902893, 0.38454783, 0.71535987, 0.04453754,\n",
       "       0.1061634 , 0.04453754, 0.0489991 , 0.68119916, 0.72086569,\n",
       "       0.71556354, 0.13380359, 0.13684272, 0.72531902, 0.16318767])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_scoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
